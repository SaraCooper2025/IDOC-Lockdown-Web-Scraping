## import modules
import pandas as pd
from datetime import timedelta
from datetime import datetime
import pytz
import os

## define time parameters -- set everything to EST because of shift timing
cst_timezone = pytz.timezone('US/Eastern')
now_cst = datetime.now(cst_timezone)
current_date = now_cst.date()
yesterday_date = current_date-timedelta(days=1)

## fetch data from the daily_scrape file generated by the IDOC web scrapper
df = pd.read_csv("./data/daily_scrape.csv")

## convert from object to datetime format
df['Date'] = pd.to_datetime(df['Date'])

## filter data so that it only includes results from the previous day
## daily_scrape should already be cleared of historical data but may include current day data -- script will run at 1:30 am EST
filtered_date = df[df['Date'].dt.date == yesterday_date]

## aggregate data by the [Facility] and [Shift] 
daily_agg = (
    filtered_date
        ## group by [Facility] and [Shift] so that each facility has 3 daily entries, one for each shift
        .groupby(['Facility','Shift'])
        .agg(Date = ('Date','max'),
            Full_Lockdown = ('Full Lockdown', 'max'),
            Partial_Lockdown = ('Partial Lockdown', 'max'),
        )
        .reset_index()
)

## save the results to an aggregate file in /data folder
daily_scrape = './data/daily_scrape.csv'
master_tracker = './data/master_tracker.csv'
os.makedirs('data', exist_ok=True)

## create new file if it does not already exist, otherwise concat results to existing file
if os.path.exists(master_tracker):
    master_df = pd.read_csv(master_tracker)
    master_df = pd.concat([master_df, daily_agg], ignore_index=True)
else:
    master_df = daily_agg

## save the updated master_tracker.csv into the /data folder
master_df.to_csv(master_tracker, index=False)

## delete data from the daily_scrape.csv so that it does not endlessly update
remaining_df = df[df['Date'].dt.date != yesterday_date]
remaining_df.to_csv(daily_scrape, index=False)
